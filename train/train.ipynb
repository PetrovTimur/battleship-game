{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T08:36:51.083783Z",
     "start_time": "2025-04-28T08:36:49.820082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np"
   ],
   "id": "5ac40aceef7c17b5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T08:44:51.701644Z",
     "start_time": "2025-04-28T08:44:51.670358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "30f6d77a07761022",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T11:40:55.739900Z",
     "start_time": "2025-04-28T11:40:55.734280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from battleship.logic.game import Field\n",
    "\n",
    "class ActionSpace:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def sample(self):\n",
    "        # Randomly sample a valid action (untried cell)\n",
    "        legal = self.env.legal_actions()\n",
    "        return random.choice(legal) if legal else 0  # fallback to 0 if no legal actions.\n",
    "\n",
    "class BattleshipEnv:\n",
    "    def __init__(self):\n",
    "        self.size = 10\n",
    "        self.n_actions = self.size * self.size\n",
    "        self.n_observations = self.size * self.size\n",
    "        self.enemy_field = None\n",
    "        self.agent_field = None\n",
    "        self.agent_view = None\n",
    "        self.done = False\n",
    "        self.remaining = None\n",
    "        self.action_space = ActionSpace(self)\n",
    "\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # one‑hot encode 0=unknown, 1=miss, 2=hit\n",
    "        v = self.agent_view\n",
    "        o0 = (v == 0).astype(np.float32)\n",
    "        o1 = (v == 1).astype(np.float32)\n",
    "        o2 = (v == 2).astype(np.float32)\n",
    "        return np.stack([o0, o1, o2], axis=0)  # shape (3,10,10)\n",
    "\n",
    "    def reset(self):\n",
    "        self.enemy_field = Field()\n",
    "        self.enemy_field.auto_place()\n",
    "        self.agent_view = np.zeros((10,10), dtype=np.int8)\n",
    "        self.done = False\n",
    "        self.remaining = sum(s.size for s in self.enemy_field.ships)\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            raise Exception(\"Game is over. Call reset().\")\n",
    "        x, y = divmod(action, self.size)\n",
    "        if self.agent_view[x, y] != 0:\n",
    "            # Invalid move (already shot here)\n",
    "            reward = -5.0\n",
    "            self.done = True\n",
    "            return self._get_obs(), reward, self.done, False, {}\n",
    "        result = self.enemy_field.check((x, y))\n",
    "        if result == 'hit':\n",
    "            self.agent_view[x, y] = 2\n",
    "            reward = 1.0\n",
    "            self.remaining -= 1\n",
    "        elif result == 'sank':\n",
    "            self.agent_view[x, y] = 2\n",
    "            reward = 5.0\n",
    "            self.remaining -= 1\n",
    "        else:\n",
    "            self.agent_view[x, y] = 1\n",
    "            reward = -1.0\n",
    "        if self.remaining == 0:\n",
    "            self.done = True\n",
    "            reward += 100.0  # Win bonus\n",
    "        return self._get_obs(), reward, self.done, False, {}\n",
    "\n",
    "    def legal_actions(self):\n",
    "        # Returns a list of valid action indices\n",
    "        return [i for i in range(self.n_actions) if self.agent_view.flat[i] == 0]"
   ],
   "id": "b825962426ed1eb2",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-28T08:51:37.759866Z",
     "start_time": "2025-04-28T08:51:37.756701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions=100):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(100, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = relu(self.layer1(x))\n",
    "        x = relu(self.layer2(x))\n",
    "\n",
    "        return self.layer3(x)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T09:36:53.238807Z",
     "start_time": "2025-04-28T09:36:53.235567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations=1, n_actions=100):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 10 * 10, 128)\n",
    "        self.fc2 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, 1, 10, 10)\n",
    "        # x = x.view(x.size(0), 1, 10, 10)\n",
    "        x = relu(self.conv1(x))\n",
    "        x = relu(self.conv2(x))\n",
    "        x = relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = relu(self.fc1(x))\n",
    "        return self.fc2(x)  # (batch, 100)"
   ],
   "id": "d76df6e4c141abe1",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T08:57:57.313952Z",
     "start_time": "2025-04-28T08:57:57.310316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "id": "77108852d4d5ba93",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T09:18:29.123456Z",
     "start_time": "2025-04-28T09:18:29.117190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "steps_done = 0\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)    # TODO: change threshold formula\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(state)\n",
    "            legal = env.legal_actions()\n",
    "            mask = torch.full(q_values.shape, float('-inf'), device=device)\n",
    "            mask[0, legal] = 0  # Only legal actions are unmasked\n",
    "            masked_q_values = q_values + mask\n",
    "            return masked_q_values.max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ],
   "id": "b2963389c1080682",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T11:34:07.838252Z",
     "start_time": "2025-04-28T11:34:07.820625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lr = 1e-4\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "batch_size = 128\n",
    "\n",
    "env = BattleshipEnv()\n",
    "n_observations = env.n_observations\n",
    "n_actions = env.n_actions\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "memory = ReplayMemory(10000)"
   ],
   "id": "b23ad5d0c28f6f30",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T09:37:47.075758Z",
     "start_time": "2025-04-28T09:37:47.067678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    # In optimize_model(), after optimizer.step():\n",
    "    loss_history.append(loss.item())"
   ],
   "id": "ee2f30f22e8ebf8a",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T11:35:05.430818Z",
     "start_time": "2025-04-28T11:35:05.422123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TARGET_UPDATE_EPISODES = 10\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "        device=device, dtype=torch.bool\n",
    "    )\n",
    "    non_final_next = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Q(s, a)\n",
    "    state_q = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Double DQN:\n",
    "    # 1) select best next actions by policy_net\n",
    "    next_policy_q = policy_net(non_final_next)\n",
    "    next_actions  = next_policy_q.argmax(dim=1, keepdim=True)\n",
    "    # 2) evaluate those actions using target_net\n",
    "    with torch.no_grad():\n",
    "        next_target_q = target_net(non_final_next).gather(1, next_actions).squeeze(1)\n",
    "\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    next_state_values[non_final_mask] = next_target_q\n",
    "\n",
    "    expected_q = reward_batch + gamma * next_state_values\n",
    "\n",
    "    loss = nn.SmoothL1Loss()(state_q, expected_q.unsqueeze(1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    loss_history.append(loss.item())"
   ],
   "id": "e6e0f3ee31ddf6b",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T11:35:03.926504Z",
     "start_time": "2025-04-28T11:35:03.924315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_episodes = 1400\n",
    "episode_durations = []\n",
    "# Add at top of file\n",
    "episode_rewards = []\n",
    "loss_history = []"
   ],
   "id": "a4067d5807ec02bd",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# single training loop, no double‐loops, no soft updates:\n",
    "for i_episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(state, device=device).unsqueeze(0)\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        obs, reward, done, _, _ = env.step(action.item())\n",
    "        total_reward += reward\n",
    "        reward_t = torch.tensor([reward], device=device)\n",
    "        next_state = None if done else torch.tensor(obs, device=device).unsqueeze(0)\n",
    "\n",
    "        memory.push(state, action, next_state, reward_t)\n",
    "        state = next_state\n",
    "\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_durations.append(t + 1)\n",
    "\n",
    "    # hard update every N episodes\n",
    "    if i_episode % TARGET_UPDATE_EPISODES == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    if i_episode % 50 == 0:\n",
    "        avg_r = sum(episode_rewards[-20:]) / 20\n",
    "        avg_l = sum(loss_history[-100:]) / min(len(loss_history), 100)\n",
    "        avg_duration = sum(episode_durations[-20:]) / 20\n",
    "        print(f\"Ep {i_episode}  avg_reward_20={avg_r:.2f}  avg_duration_20={avg_duration:.2f}  avg_loss_100={avg_l:.4f}\")"
   ],
   "id": "5fd870641f78d3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i_episode in range(num_episodes):\n",
    "    print(i_episode)\n",
    "    state, info = env.reset()\n",
    "\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], dtype=torch.float32, device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*tau + target_net_state_dict[key]*(1-tau)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            # plot_durations()\n",
    "            break"
   ],
   "id": "e7cd76830e2414b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T09:39:51.340639Z",
     "start_time": "2025-04-28T09:39:51.259317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ],
   "id": "c1e0cc4af63df9dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T08:54:49.270096Z",
     "start_time": "2025-04-28T08:54:49.241488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = BattleshipEnv()\n",
    "num_episodes = 1\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    while not done:\n",
    "        # action = select_action(state)\n",
    "        # print(action)\n",
    "        print(policy_net(state).max(1).indices.view(1, 1))\n",
    "        obs, reward, done, truncated, _ = env.step(action.item())\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "    print(f\"Episode {episode+1}: Steps={steps}, Total Reward={total_reward}\")"
   ],
   "id": "2498028350682e21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[77]], device='cuda:0')\n",
      "tensor([[77]], device='cuda:0')\n",
      "Episode 1: Steps=2, Total Reward=-6.0\n"
     ]
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
